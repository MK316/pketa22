,Text_num,N_of_words,Text
0,text1,785,"Before the age of the smartphone, aspiring photographers had to learn how to use high-tech cameras and photographic techniques. Not everyone had cameras, and it took skill and a good eye to capture and create a great photograph. Today, with the huge range of camera apps on our smartphones, we are all amateur photographers. And pretty good ones, too: The quality of smartphone images now nearly equals that of digital cameras.  The new ease of photography has given us a tremendous appetite for capturing the magical and the ordinary. We are obsessed with documenting everyday moments, whether it’s a shot of our breakfast, our cat – or our cat’s breakfast. And rather than collect pictures in scrapbooks, we share, like, and comment on them with friends and strangers around the globe.  Even photojournalists are experimenting with cell phones because their near invisibility makes it easier to capture unguarded media. They can now act as their own publishers – reaching huge audiences via social media sites such as Instagram. A photograph taken in New York can get a response from someone in Lagos within a second of being uploaded.  In the past, magazines published unforgettable photos of important people and global events that captured our imaginations. These photos had the power to change public opinion – even the course of history. But if there are fewer memorable images today, it’s not because there are fewer good images: It’s because there are so many. No one image gets to be special for long.  Cameras are everywhere – a situation that is transforming the way we experience dramatic events. When there are major political events or natural disasters, it is ordinary citizens with cell phones – not photojournalists – who often provide the first news images. Quality still matters, but it’s less important than what’s instantly shared.  As people everywhere embrace photography and the media make use of citizen journalist, professional standards appear to be shifting. In the past, most people trusted photojournalists to accurately represent reality. Today, however, digital images can be altered in ways the naked eye might never notice. Any image can be altered to create an “improved” picture of reality. The average viewer is left with no way to assess the accuracy of an image except through trust in a news organization or photographer.  The question of the accuracy of images gets even trickier when photojournalists start experimenting with camera apps – like Flickr or Instagram – which encourage the use of filters. Images can be colored, brightened, faded, and scratched to make photographs more artistic, or to give them an antique look. Photojournalists using camera apps to cover wars and conflicts have created powerful images – but also controversy. Critics worry that antique-looking photographs romanticize war, while distancing us from those who fight in them.  Yet, photography has always been more subjective than we assume. Each picture is a result of a series of decisions – where to stand, what lens to use, and what to leave in or out of the frame. Does altering photographs with camera app filters make them less true?  There’s something powerful and exciting about the experiment that the digital age has forced upon us. These new tools make it easier to tell our own stories, and they give others the power to do the same. Many members of the media get stuck on the same stories, focusing on elections, governments, wars, and disasters. In the process, they miss out on the less dramatic images of daily life that can be just as revealing and relevant.  The increase in the number of photographs and photographers might even be good for democracy itself. Hundreds of millions of potential citizen journalists make the world smaller and help keep leaders honest. People can now show what they are up against, making it increasingly difficult for governments to hide their actions. If everyone has a camera, Big Brother isn’t the only one watching.  Who knows? Our obsession with documentation and constantly being connected could lead to a radical change in our way of being. Perhaps we are witnessing the development of a universal visual language. It’s one that could change the way we relate to each other and the world. Of course, as with any language, there will be those who produce poetry and those who make shopping lists.  It’s not clear whether this flowering of image-making will lead to a public that better appreciates and understands images. Or will it simply numb us to the profound effects a well-made image can have? Regardless, the change is irreversible. Let’s hope the millions of new photographs made today help us see what we all have in common, rather than what sets us apart."
1,text2,577,"The ability of some species to create light, known as bioluminescence, is both magical and commonplace. Magical, because of its glimmering beauty. Commonplace, because many life forms can do it. On land the most familiar examples are fireflies, flashing to attract mates on a warm summer night. But there are other luminous land organisms, including glow-worms, millipedes, and some ninety species of fungus. Even some birds, such as the Atlantic puffin, have beaks that glow in the dark. But the real biological light show takes place in the sea. Here, an astonishing number of beings can make light. Some, such as ostracods, are like ocean fireflies, using flashes of light to attract mate. There are also glowing bacteria, and light-making fish, squid, and jellyfish. Indeed, of all the groups of organisms known to make light, more than four-fifths live in the ocean. As a place to live, the ocean has a couple of peculiarities. Firstly, there is almost nowhere to hide, so being invisible is very important. Secondly, as you descend, sunlight disappears. At first, red light is absorbed. Then the yellow and green parts of the spectrum disappear, leaving just the blue. At two-hundred meters below the surface, the ocean becomes a kind of perpetual twilight, and at six-hundred meters the blue fades out too. In fact, most of the ocean is as black as the night sky. These factors make light uniquely useful as a weapon or a veil. Hiding with light In the ocean’s upper layers, where light penetrates, creatures need to blend in to survive. Any life form that stands out is in danger of being spotted by predators, especially those swimming below, looking up. Many life forms solve this problem by avoiding the light zone during the day. Others, such as jellyfish and swimming snails, are transparent, ghostlike creatures, almost impossible to see. Other sea species use light to survive in the upper layers, but how? Some, such as certain shrimp and squid, illuminate their bellies to match the light coming from above. This allows them to become invisible to predators below. Their light can be turned on and off at will, some even have a dimmer switch. For example, certain types of shrimp can alter how much light they give off, depending on the brightness of the water around them. If a cloud passes overhead and briefly blocks the light, the shrimp will dim itself accordingly. But if the aim is to remain invisible, why do some creatures light up when they are touched, or when the water nearby is disturbed? A couple of reasons. First, a sudden burst of light may startle a predator, giving the prey a chance to escape. Some kinds of deep-sea squid, for example, give a big squirt of light before darting off into the gloom. Second, there is the principle of ‘the enemy of my enemy is my friend.’ Giving off light can help summon the predator of your predator. Known as the burglar alarm effect, this is especially useful for tiny life forms, such as dinoflagellates, that cannot swim fast. For such small beings, water is too viscous to allow a quick getaway, it would be like trying to swim through syrup. Instead, when threatened by a shrimp, for example, these organisms light up. The flashes attract larger fish that are better able to spot, and eat, the shrimp. The chief defense for these tiny organisms is therefore not fight or flight, but light. "
2,text3,854,"Genetic engineering (GE) of crops and animals through the manipulation of DNA is producing a revolution in food production.The potential to improve the quality and nutritional value of the food (that)  we eat seems unlimited. Such potential benefits notwithstanding, critics fear that genetically engineered products – so-called biotech foods – are being rushed to market before their effects are fully understood. On the other hand, the techniques of genetic engineering are new and different.Conventional breeders always used plants or animals that were related, or genetically similar. In so doing, they transferred tens of thousands of genes.In contrast, today’s genetic engineers can transfer just a few genes at a time between species that are distantly related, or not related at all. There are surprising example: Rat genes have been inserted into lettuce plants to make a plant that produces vitamin C. Moth genes have been inserted into apple trees to add disease resistance. The purpose of conventional and modern techniques is the same – to insert genes from an organism that carries a desired trait into one that does not. Several dozen biotech food crops are currently on the market, among them varieties of corn, soybeans, and cotton. Most of these crops ate engineered to help farmers deal with common farming problems such as weeds, insects, and disease. Are biotech foods safe for humans? As far as we know. So far, problems have been few. In fact, according to a 2016 report from the National Academy of Sciences in the United States, ""No differences have been found that indicate a higher risk to human health and safety from these GE foods than from their non-GE counterparts. Some GE foods might even be safer than non-GE foods. Corn damaged by insects often contains high levels of fumonisins-toxins² (that grow in the wounds of the damaged corn.) Lab tests have linked fumonisins with cancer in animals. Studies show that most corn modified for insect resistance has  lower levels of fumonisins than conventional corn damaged by insects. However, biotech foods have had problems in the past. One such problem occurred in the mid-1990s, when soybeans were modified using genes from a nut. The modified soybeans contained a protein that causes reactions in humans who are allergic to nuts. While this protein was discovered before any damage was done, critics fear that other harmful proteins created through genetic modification may slip by unnoticed. Moving genes across dramatically different species, such as rats and lettus, also makes critics nervous. They fear something could go wrong either in the function of the inserted gene or in the function of the host DNA, with the possibility of unexpected health effects. Most scientists agree that the main safety issues of GE crops involve not people but the environment. Allison Snow, a plant ecologist at Ohio State University, worries that GE crops are being developed too quickly and released before they’ ve been adequately tested. On the other hand, advocates of GE crops argue that some genetically modified plants may actually be good for the land, by offering an environmentally friendly alternative to pesticides, which can pollute water and harm animals. Far fewer pesticides need to be applied to cotton plants that have been genetically modified to produce their own natural pesticides. While applied chemical pesticides kill nearly all the insects in a field, biotech crops with natural pesticides only harm insects that actually try to eat those crops. Can biotech foods help feed the world? “Eight hundred million people (on this planet) are malnourished,” says Channapatna Prakash, a native of India and a scientist (at Tuskegee University’s Center) (for Plant Biotechnology  Research) (in the U.S.A), and the number continues to grow.  Prakash and many other scientists argue that genetic modification can help address the urgent problems of food shortage and hunger (by increasing crop quantities). Crops can be engineered to grow in area with harsh, dry climates or in soils not usually suitable for farming. According to the World Health Organization, an estimated 250million children in the world /suffer from vitaminA deficiency Between 250,000 and 500,000 go blind every year as a result, with half of those children dying within a year of losing their sight. “Golden rice” - a biotech variety named for its yellow colors - is thought by some to be a potential solution to the suffering and illness caused by vitamin A deficiency Other experts, however, claim that the biotechnology industry  has exaggerated the benefits of golden rice. “Golden rice alone won’t greatly diminish vitamin A deficiency,” says Professor Marion Nestle of New York University. “Beta-carotene, which is already widely available in fruit and vegetables, isn’t converted to vitamin A when people are malnourished. Golden rice does not contain much beta-carotene, and whether it will improve vitamin A levels remains to be seen.” Whether biotech foods will deliver on their promise of eliminating world hunger and improving the lives of all remains to be seen. Their potential is enormous, yet they carry risks. If science proceeds with caution, testing new products thoroughly and using sound judgment, the world may avoid the dangers of genetic modification while enjoying its benefits."
3,text4,1007,"Everybody loves a good story, and when it's finished, this may be the greatest one ever told. It begins in Africa with a group of people. There are perhaps just a few hundred, surviving by hunting animals and gathering fruits, vegetables, and nuts. It ends about 200,000 years later, with their seven billion descendants spread across the Earth.  In between is an exciting tale of survival, movement, isolation, and conquest, most of it occurring before recorded history. Who were those first modern people in Africa? What routes did they take when they left their home continent to expand into Europe and Asia? When and how did humans reach the Americas? For decades, the only proof was found in a small number of scattered bones and artifacts that our ancestors had left behind. In the past 20 years, however, DNA technologies have allowed scientists to find a record of ancient human migrations in the DNA of living people.   ""Every drop of human blood contains a history book written in the language of our genes,"" says population geneticist Spencer Wells. The human genetic code, or genome, is 99.9 percent identical throughout the world. The bulk of our DNA is the same. However, the remainder is responsible for our individual differences -- in eye color or disease risk, for example. On very rare occasions, a small change -- called a mutation -- can occur. This can then be passed down to all of that person's descendants. Generations later, finding that same mutation in two people's DNA indicates that they share the same ancestor. By comparing mutations in many different populations, scientists can trace their ancestral connections.  These ancient mutations are easiest to tract in two places. One is in DNA that is passed from mother to child (called mitochondrial DNA, or mtDNA). The other is in DNA that travels from father to son (known as the Y chromosome, the part of DNA that determines a child will be a boy). By comparing the mtDNA and Y chromosomes of people from various populations, geneticists can get a rough idea of where and when those groups separated in the great migrations around the planet.  In the mid-1980s, a study compared mtDNA from people around the world. It found that people of African descent had twice as many genetic differences from each other than did others. Because mutations seem to occur at a steady rate over time, scientists concluded that modern humans must have lived in Africa at least twice as long as anywhere else. They now calculate that all living humans maternally descend from a single woman who lived roughly 150,000 years ago in Africa, a ""mitochondrial Eve."" If geneticists are right, all of humanity is linked to Eve through an unbroken chain of mothers. This Eve was soon joined by ""Y-chromosome Adam,"" the possible genetic father of us all, also from Africa. DNA studies have confirmed that all the people on Earth can trace their ancestry to ancient Africans.  What seems certain is that at a remarkably recent date -- probably between 50,000 and 70,000 years ago -- one small group of people, the ancestors of modern humans outside of Africa, left Africa for western Asia. They either migrated around the wider northern end of the Red Sea, or across its narrow southern opening.  Once in Asia, genetic evidence suggests, the population split. One group stayed temporarily in the Middle East, while the other began a journey that would last tens of thousands of years. Moving a little farther with each new generation, they followed the coast around the Arabian Peninsula, India, and Southeast Asia, all the way to Australia. ""The movement was probably imperceptible,"" says Spencer Wells. ""It was less of a journey and probably more like walking a little farther down the beach to get away from the crowd.""  Archeological evidence of this 13,000-kilometer migration from Africa to Australia has almost completely vanished. However, genetic traces of the group that made the trip do exist. They have been found in the DNA of indigenous peoples in Malaysia, in Papua New Guinea, and in the DNA of nearly all Australian aborigines. Modern discoveries of 45,000-year-old bodies in Australia, buried at a site called Lake Mungo, provide physical evidence for the theories as well.  People in the rest of Asia and Europe share different but equally ancient mtDNA and Y-chromosome mutations. These mutations show that most are descendants of the group that stayed in the Middle East for thousands of years before moving on. Perhaps about 40,000 years ago, modern humans first advanced into Europe.  About the same time as modern humans pushed into Europe, some of the same group that had paused in the Middle East spread east into Central Asia. They eventually reached as fas as Siberia, the Korean Peninsula, and Japan. Here begins one of the last chapters in the human story -- the peopling of the Americas. Most scientists believe that today's Native Americans descend from ancient Asians who crossed from Siberia to Alaske in the last ice age. At that time, low sea levels would have exposed a land bridge between the continents. Perhaps they -- only a few hundred people -- were travelling along the coast, moving from one piece of land to the next, between a freezing ocean and a wall of ice. ""A coastal route would have been the easiest way in,"" says Wells. ""But it still would have been a hell of a trip."" Once across, they followed the immense herds of animals into the mainland. They spread to the tip of South America in as little as a thousand years.  Genetic researchers can only tell us the basic outlines of a story of human migration that is more complex than any ever written. Many details of the movements of our ancestors and their countless individual lives can only be imagined. But thanks to genetic researchers -- themselves descendants of mtDNA Eve and Y-chromosome Adam -- we have begun to unlock important secrets about the origins and movements of our ancient ancestors."
4,text5,1002,"About 9,500 years ago, ancient accountants in Sumer invented a way to keep tract of farmers' crops and livestock. They began using small pieces of baked clay, almost like the tokens used in board games today. One piece might signify a measure of grain, while another with a different shape might represent a farm animal or a jar of olive oil. Those little ceramic shapes might not seem to have much in common with today's $100 bill - or with the credit cards and online transactions that have rapidly taking the place of cash - but the roots of our modern methods of payment lie in those Sumerian tokens. Such early accounting tools evolved into a system of finance and into money itself: a symbolic representation of value that can be transferred from one person to another as payment for goods or services. Since ancient times, humans have used items to represent value - from stones to animal skins, to whale teeth. In the ancient world, people often relied upon symbols that had tangible value in their own right. The ancient Chinese made payments with cowrie shells, which were prized for their beauty as materials for jewelry. As Glyn Davies notes in his book A History of Money from Ancient Times to the Present Day, cowrie shells are durable, easily cleaned and counted, and defy imitation or counterfeiting. But eventually there arose a new, universal currency: gold. The gleaming metal could be combined with other metals at high temperatures to create alloys, and was easy to melt and hammer into shapes. It became the raw material for the first coins, created in Lydia (present-day Turkey) around 2,700 years ago. Lydian coins didn't look much like today's coinage. They were irregular in shape and size and didn't have values inscribed on them; instead, they used a stamped image to indicate their weight and value. The result, explains financial author Kabir Sehgal, was an economic system in which ""you knew the value of what you had, and what you could buy with it."" Unlike modern money, ancient coins were what economists call full-bodied or commodity money. Their value was fixed by the metal in them. Money's convenience made it easier for ancient merchants to develop large-scale trade networks, in which spices and grain could be bought and sold across distances of thousands of kilometers This led to the first foreign exchanges. In the ancient Greek city-state of Corinth, banks were set up where foreign traders could exchange their own coins for Corinthian ones. In the centuries that followed, trade routes forged more cultural connections between nations and regions. Besides exchanging money and goods, traders also spread religious beliefs, knowledge, and new inventions, creating connections among far-flung cultures. The dangers of moving money and goods over distances - whether from storms at sea or bandits and pirates - led humans to develop increasingly complex economic organizations. In the 1600s, investors gathering in London coffeehouses began to underwrite traders and colonists heading to the New World, financing their voyages in exchange for a share of the crops or goods they brought back. Investors tried to reduce their risk by buying shares of multiple ventures. It was the start of a global economy in which vast quantities of products and money began to flow across borders in search of profit. By the 1700s, the global economy had grown so much that it was inconvenient to transport and store large quantities of coins. Several societies therefore shifted toward paper currency. The earliest paper bills were literally receipts that gave the bearer ownership of gold or silver coins that could be collected upon demand. But as Lloyd Thomas explains in his book, Money, Banking and Financial Markets, bankers eventually realized that many people simply used their notes rather than redeeming them for gold. It meant that the bankers didn't actually need to have enough gold on hand to cover all the notes they issued. That revelation, Thomas says, eventually led to the concept of flat money, which governments issue today. In contrast to commodity money, today's money has value essentially because a government says that it does. Its purchasing power remains relatively stable because the government controls the supply. That's why a U.S.$100 bill is worth $100, even though it only contains a few cents worth of raw materials. It's a system with an important advantage, in that human judgment - rather than how much gold has been dug out of the ground - determines the amount of money in circulation. On the other hand, this can become a disadvantage. If a government decides to issue too much money, it can trigger an inflationary spiral that raises the price of goods and services. By the 20th century, new methods of payment had begun to emerge as alternatives to cash. In the 1920s, oil companies and hotel chains began to issue credit cards. These enabled customers to make purchases and pay what they owed later. In 1950, Diners Club International issued the first universal credit card, which could be used to purchase things at a variety of places. Using plastic to make purchases eventually proved more convenient that bills, coins, or even checks. In 2009, yet another high-tech successor to money emerged: Bitcoin. Bitcoins are a sort of unofficial virtual Internet currency. They aren't issued or even controlled by governments, and they exist only in the cloud or on a person's computer. Parag Khannam a financial policy expert, explains, ""The real future is technology as money. That's what Bitcoin is about."" From the clay tokens of Sumer to today's virtual currencies, the evolution of money has helped drive the development of civilization. Money makes it easier not only to buy and sell goods, but also to connect with the world, enabling traders to roam across continents, and investors to amass wealth. It is a type of language that we all speak. From the humblest shop clerk to the wealthiest Wall Street financier, money exerts a powerful influence upon us all."
